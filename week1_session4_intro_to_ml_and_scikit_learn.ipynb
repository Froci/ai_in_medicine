{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI in Medicine: Data Science - Machine Learning\n",
    "## Python programming: Machine learning using *scikit-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tutor:** Roshan Prakash Rane, AG Ritter, Charité - Universitätsmedizin Berlin (roshan-prakash.rane@charite.de)\n",
    "- **Target audience**: Medical students from Charité\n",
    "- **Course date**: February 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you are using this notebook outside of Google Colab,**please ensure that you're using a Python 3.6 or higher kernel**. You can check that in the top right corner of your browser window. If you're using a different Version, go to the Tab \"Kernel\" --> \"Change Kernel\" and select \"Python 3.6\" or higher. <br>\n",
    "\n",
    "**Executing the below cell should return a version higher than 3.6.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aims of this session\n",
    "\n",
    "This session will introduce basic concepts of **machine learning** using a practical example and a programming tutorial. We will use the Python programming language. Specifically, we will learn to use Python's machine learning package '*scikit-learn*' to train different machine learning models and evaluate their performance. \n",
    "\n",
    "We will be using the [diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database). We will train few machine learning models to predict if a subject has diabetes from characteristics such as blood pressure, blood glucose level, BMI, age  etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning goals\n",
    "\n",
    "By the end of this session, you should be familiar with:\n",
    "\n",
    "- Reading a dataset, exploring the different data columns and cleaning it for machine learning.\n",
    "- Cross validation: Why we split our dataset into 'training' data and 'test' data and how to do it.\n",
    "- Training machine learning models.\n",
    "- Compare the performance of different machine learning models and determine which one is better.\n",
    "- Interpreting the predictions of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References\n",
    "\n",
    "Documentation for python libraries used in this notebook:\n",
    "\n",
    "- https://numpy.org/doc/\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://pandas.pydata.org/pandas-docs/stable/\n",
    "- https://matplotlib.org/\n",
    "\n",
    "Documentation for classifiers:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Logistic_regression\n",
    "- https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "Documentation for metrics used:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
    "- https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "- https://en.wikipedia.org/wiki/F1_score\n",
    "- https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "Further learning material:\n",
    "\n",
    "- StatQuest's Machine learning lectures on youtube: ([click here](https://www.youtube.com/playlist?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF))<br>(Episodes: Introduction, Cross-validation, Confusion Matrix, Sensitivity and Specificity, k-nearest neighbor, linear / logistic regression, support vector machines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Theory\n",
    "The theory will be covered alongside as we walk-through the practical sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical\n",
    "5.1 Data loading and exploration<br>\n",
    "5.2 Data preprocessing<br>\n",
    "5.3 Classification or regression?<br>\n",
    "5.4 Splitting the data into 'training' set and a 'test' set<br>\n",
    "5.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data loading and exploration\n",
    "\n",
    "* The dataset we will be exploring in this tutorial is from the Pima Indians Diabetes Database that is freely [available here](https://www.kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "* The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements.\n",
    "* We will be using a subset of the larger database for ease-of-use. \n",
    "* Our subset is selected such that, all subjects in it are females, at least 21 years old, and of Pima Indian heritage.\n",
    "\n",
    "Source publication for reference:<br>\n",
    "*Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from the github to the Colab environment\n",
    "!mkdir -p data # create a new folder called 'data'\n",
    "!wget -q -P data https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/data/diabetes.csv # download the file to 'data' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load the dataset and try to understand the different columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the Pandas, NumPy and matplotlib libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file as a pandas dataframe\n",
    "df = pd.read_csv('data/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the loaded dataframe \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe,\n",
    "- the dataset has 768 subjects and each subject have 9 variables:\n",
    "    1. Pregnancies\n",
    "    2. Glucose \n",
    "    3. BloodPressure \n",
    "    4. SkinThickness \n",
    "    5. Insulin \n",
    "    6. BMI \n",
    "    7. DiabetesPedigreeFunction \n",
    "    8. Age \n",
    "    9. Outcome\n",
    "- The last column \"Outcome\" denotes the diabetic information. A value of '1' here denotes that the subject has diabetes.\n",
    "- The rest of the 8 columns are medical predictor variables that might or might not be associated with diabetes.\n",
    "\n",
    "**Our task here is to use these 8 medical predictor variables and try and predict if the subject has diabetes or not** (i.e. to predict the 'outcome' variable):\n",
    "\n",
    "<img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/MLflowchart.png\">\n",
    "\n",
    "*([link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/MLflowchart.png))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \n",
    "1. What variables among these do you think are the most telling medically for diabetes diagnosis? \n",
    "2. Which variables, according to you, will the machine learning model find most useful for diabetes diagnosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualize the 'distribution' of the diabetes column in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the subject with and without diabetes\n",
    "label = df[\"Outcome\"].astype(int) # first, select the outcome column from the dataframe\n",
    "label_counts = label.value_counts()  # next use the pandas 'value_counts()' method \n",
    "# print the result\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `values_counts()` method, we can print how many of each class we have and calculate their percentages. <br>\n",
    "\n",
    "Now, let's make these numbers more intuitive by plotting a 'bar' graph showing their counts.\n",
    "\n",
    "We can use the `plot()` function of pandas to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = label_counts.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we can improve this graph a bit more and make it readable for anyone else who would look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = label_counts.plot(kind=\"bar\", \n",
    "                       title=\"Count of diabetic vs healthy subjects\",\n",
    "                       ylabel=\"number of subjects\", \n",
    "                       rot=0)\n",
    "tick_names = ax.set_xticklabels([\"healthy\", \"diabetic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the distributions of the different medical variables that will be provided as input to our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a canvas on which 1 x 4 graphs can be drawn\n",
    "f, axes = plt.subplots(1, 4, sharey=True, figsize=(16,4))\n",
    "\n",
    "df.plot(y=\"Age\", kind=\"hist\", ax=axes[0])\n",
    "df.plot(y=\"Glucose\", kind=\"hist\", ax=axes[1])\n",
    "df.plot(y=\"DiabetesPedigreeFunction\", kind=\"hist\", ax=axes[2])\n",
    "df.plot(y=\"Pregnancies\", kind=\"hist\", ax=axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now it's your turn. Similar to what we did in the previous cell, plot the distributions of the remaining 4 variables in the dataset:\n",
    "1. BMI\n",
    "2. SkinThickness\n",
    "3. BloodPressure\n",
    "4. Insulin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# hint: start by copying the code from the above cell. Next, modify the column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- f, axes = plt.subplots(1, 4, sharey=True, figsize=(16,4))\n",
    "\n",
    "df.plot(y=\"BMI\", kind=\"hist\", ax=axes[0])\n",
    "df.plot(y=\"SkinThickness\", kind=\"hist\", ax=axes[1])\n",
    "df.plot(y=\"BloodPressure\", kind=\"hist\", ax=axes[2])\n",
    "df.plot(y=\"Insulin\", kind=\"hist\", ax=axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a very rare occurance that datasets are clean, complete, and in the right shape.  Either due to human errors or due to systemic issues, in practice, you will come across datasets that need to be preprocessed before they can be used to train a machine learning model.\n",
    "\n",
    "Some of the prominent preprocessing steps include:\n",
    "* Dropping non-numeric values\n",
    "* Cleaning unneeded columns\n",
    "* Converting measurement units\n",
    "* Reducing noise using smoothing functions\n",
    "* Standardizing and normalizing variable values: covered in the theory session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Do you find any such discrepancies in our dataset so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some subjects have 'Glucose' value as '0' which can't be the case. Maybe the data was not collected for these subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the count of subjects with Glucose == 0\n",
    "glucose_zero = (df['Glucose'] == 0)\n",
    "glucose_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_zero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop these subjects from our data as they are probably not very reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the subjects who have a 'Glucose' value \n",
    "df_clean = df[(df['Glucose'] != 0)]\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same case with 'BloodPressure' where some subjects have '0'. Let's remove them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[(df_clean['BloodPressure'] != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Similarly, lets also remove subjects with BMI=0 as those are also probably incoherent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  df_clean = df_clean[(df_clean['BMI'] != 0)] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Classification or regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to look at our data again. <br>\n",
    "This time, let's use a 'scatter plot' to compare how 2 input variable relate with the output 'diabetes' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "# set a red color for diabetic subjects and blue for healthy subjects\n",
    "label = df_clean['Outcome'].map({0:'blue', 1:'red'})\n",
    "\n",
    "# plot BMI vs BloodGlucose \n",
    "ax1 = df_clean.plot(x=\"BMI\", y=\"Glucose\", c=label, kind=\"scatter\", ax=axes[0])\n",
    "ax1.legend([\"Diabetic\", \"Healthy\"])\n",
    "\n",
    "# plot DiabetesPedigreeFunction vs BloodGlucose \n",
    "ax2 = df_clean.plot(x=\"DiabetesPedigreeFunction\", y=\"Glucose\", c=label, kind=\"scatter\", ax=axes[1])\n",
    "ax2.legend([\"Diabetic\", \"Healthy\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is our task a classification task or a regression task?\n",
    "\n",
    "<img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/classandregress.png\" width=\"700\" />\n",
    "\n",
    "*([link to source](http://tonyeiyalla.com/images/classandregress.png))*\n",
    "\n",
    "**Discussion:** \n",
    "1. Do you see a correlation between the \"BMI\" and \"Glucose\" scores?\n",
    "2. What about between \"BMI\" and \"DiabetesPedigreeFunction\"?\n",
    "3. If you had to draw a line to differentiate between diabetic and healthy subjects in the first plot, where would you put it?\n",
    "4. Are we doing classification or regression?\n",
    "5. If we were trying to predict 'Glucose' from 'BMI', would we be doing a classification or a regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the 'X' and y for our machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean[[\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]].values\n",
    "y = df_clean[[\"Outcome\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Splitting the data into 'training' set and 'test' set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need a test set?\n",
    "\n",
    "Over-fitting problem in a classification task vs a regression task\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/tuning.png\" width=\"300\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/fitting_data.png\" width=\"800\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "*([link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/tuning.png))*\n",
    "*([link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/fitting_data.png))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we split our dataset into 2 subsets: \n",
    "1. A larger subset on which we will train our model called the 'training' set. \n",
    "2. A smaller subset on which we will 'test' the model. It is important that we evaluate our model on a subset of the data it has never seen before to ensure that we are not overfitting the data and that our model can generalize well to unseen data.\n",
    "\n",
    "Let us use 20% of our data as the test set. The remainding 80% are used to train the classifer. These ratios may vary depending on the size of the dataset we are using, but 20% to 80% is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many subject form '80%' in our data\n",
    "round(len(X)*80/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[:579]\n",
    "x_test = X[579:]\n",
    "\n",
    "y_train = y[:579]\n",
    "y_test = y[579:]\n",
    "\n",
    "(x_train.shape), (x_test.shape), (y_train.shape), (y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a machine learning library called '*sklearn*' that provides several convinient functions for machine learning:<br>\n",
    "<img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/Scikit_learn_logo.png\" width=\"300\" /> \n",
    "*([link to source](https://en.wikipedia.org/wiki/Scikit-learn#/media/File:Scikit_learn_logo_small.svg))* <br>\n",
    "\n",
    "*sklearn* library has a function called `train_test_split()` that can be used for splitting our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function for splitting our data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the features and labels intro training and test sets by setting the test_size variable to 20%\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "(x_train.shape), (x_test.shape), (y_train.shape), (y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train our Models\n",
    "\n",
    "Lets use try 3 different machine learning models for learning on our training data:\n",
    "1. Linear Support Vector Machine Classifier (LinearSVC)\n",
    "2. Non-linear Support Vector Machine Classifier (SVC)\n",
    "3. Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1 Linear Support Vector Machine Classifier (LinearSVC)\n",
    "<img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/hyperplane.png\" width=\"400\" /> \n",
    "\n",
    "*([link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/hyperplane.png))* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model classe from sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create a model (also called creating an 'instance' of a model class in programming lingo) \n",
    "linsvc = LinearSVC() #max_iter=200\n",
    "\n",
    "# fit the model to our train data using a class method\n",
    "linsvc.fit(x_train, y_train) #.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the warning message from python, it appears as though our linear SVC did not converge. This may imply that our data is not linearly separable and that our other two classifiers might be better suited the for the classification task. Lets forget the linear SVC and try our the other classifiers.\n",
    "\n",
    "#### 5.5.2 Non-linear Support Vector Machine Classifier (SVC) \n",
    "Linearly seperable task vs non-linearly seperable task:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/linear_sep.png\" width=\"300\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/non-linear_sep.png\" width=\"300\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "(*[link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/linear_sep.png)*)\n",
    "(*[link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/non-linear_sep.png)*)\n",
    "\n",
    "<img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/sphx_glr_plot_iris_svc_001.png\" width=\"700\" />\n",
    "\n",
    "(*[link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/sphx_glr_plot_iris_svc_001.png)*)\n",
    "\n",
    "This is good for separating non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model classe from sklearn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate an object of the model class\n",
    "svc = SVC(probability=True) # We set probability to True when instantiating our SVC class in order to get a probability estimate of the labels.\n",
    "\n",
    "\n",
    "# fit the model to our train data using a class method\n",
    "svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.3 Logistic Regression Classifier\n",
    "\n",
    "A logistic regression classifier is also good for separating non-linearly separable data. We set the max iterations of our logistic classigier to 200 when we instantiate our class because it did not converges with the default value of 100.\n",
    "\n",
    "<img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/logistic_reg.png\" width=\"400\" /> *([link to source](https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/logistic_reg.png))* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate an object of the model class\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# fit the model to our train data using a class method\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two model that have been fit to our training data, we can use our test data to evaluate them.\n",
    "\n",
    "### 5.6 Evaluate our Models\n",
    "\n",
    "\n",
    "#### 5.6.1 Make predictions\n",
    "\n",
    "Next, we can make predictions on the test set that we will later compare to the respective true labels to evaluate of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful functions from the metrics module to evaluate our model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the predict() method from our model classes to predict labels given the test set of features x_test.\n",
    "y_pred_svc = svc.predict(x_test)\n",
    "y_pred_log = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2 Accuracy\n",
    "\n",
    "$\\text{accuracy} = \\frac{tp + tn}{tp + fp + tn + fn}$, is a measure of how well our classifier can determine the true labels of inputs. <br><br>\n",
    "Here we will compute the accuracies of both of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using the model class method score() to return the accuracy of predictions from each model.\n",
    "print('Accuracy of Support Vector classifier on test set: {:.1f}%'.format(svc.score(x_test, y_test)*100))\n",
    "print('Accuracy of logistic regression classifier on test set: {:.1f}%'.format(logreg.score(x_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although accuracy is a common metric, it often does not tell the whole story.\n",
    "\n",
    "We need other ways to asses how well our classifier performs.\n",
    "\n",
    "#### 5.5.3 Outcomes of a classifier\n",
    "\n",
    "_Successful predictions_ are only one of the possible outcomes of a prediction from a classifier. These outcomes can be generalized using the following four classes:\n",
    "\n",
    "- True positives\n",
    "- False positives\n",
    "- True negatives\n",
    "- False negatives\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/confusion_matrix.png\" width=\"500\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://github.com/volkamerlab/ai_in_medicine/raw/update-2021.02/images/confusion_matrix_pregnancy.png\" width=\"500\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "(*[link to source](https://dzone.com/articles/understanding-the-confusion-matrix)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.4 Confusion Matrices\n",
    "\n",
    "We can visualize the distribution of prediction classes predicted using a classifier by plotting a confusion matrix. We now use the plot_confusion_matrix function we imported above to visualize the distribtion of True Positives, False Positives, False Negatives, True Negatives for both of our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the documentation to know what variables to use!\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15, 5))\n",
    "plot_confusion_matrix(svc, x_test, y_test, normalize='all', ax=ax1)\n",
    "ax1.set_title('Confusion Matrix - SVC', fontsize=20)\n",
    "plot_confusion_matrix(logreg, x_test, y_test, normalize='all', ax=ax2)\n",
    "ax2.set_title('Confusion Matrix - LR', fontsize=20)\n",
    "\n",
    "# create directory to contain results\n",
    "!mkdir -p results/week1_session4_intro_to_ml_and_scikit-learn\n",
    "fig.savefig(\n",
    "    'results/week1_session4_intro_to_ml_and_scikit-learn/confusion_matrices'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.5 Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The ROC curve is a graph that shows the performance of a classification model at all thresholds. It is a common choice for assessing a binary classifier. The ROC curve is a plot of the True Positive Rate (recall) vs the False Positive Rate. The dotted red line indicates the performance of a random classifier that has a 50% chance of outputting either label. The area under the curve (AUC) of the ROC curve is a measure that tells us how well our classifier can distinguish between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Figure, Axes objects and set figure dimensions\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15, 5))\n",
    "\n",
    "\n",
    "# compute and plot the AUC and ROC values for the support \n",
    "# vector classifier\n",
    "svc_roc_auc = roc_auc_score(y_test, svc.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svc.predict_proba(x_test)[:,1])\n",
    "\n",
    "\n",
    "ax1.plot(\n",
    "    fpr, tpr, label='SVC - RBF Kernel (area = %0.2f)' % svc_roc_auc\n",
    ")\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Receiver operating characteristic')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid()\n",
    "\n",
    "\n",
    "# compute and plot the AUC and ROC values for the logistic \n",
    "# regression classifier\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])\n",
    "\n",
    "\n",
    "ax2.plot(\n",
    "    fpr, \n",
    "    tpr, \n",
    "    label='Logistic Regression (area = %0.2f)' % logit_roc_auc\n",
    ")\n",
    "ax2.plot([0, 1], [0, 1],'r--')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('Receiver operating characteristic')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid()\n",
    "\n",
    "fig.savefig('results/week1_session4_intro_to_ml_and_scikit-learn/roc');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Which model do you think is better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, data scientists also use n-fold cross-validation and permutation tests to better compare models, tune hyperparameters, and get better estimates for our metrics. <br>\n",
    "This will result in a clearer picture of how the models preform, and which models to use for classification of novel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Lets briefly revisit some topics covered in this module:\n",
    "\n",
    "- Machine learning is a task of learning models that can map input variables to desired outputs with high precision; features to labels\n",
    "    - classification: outputs are discrete variables or classes\n",
    "    - regression: outputs are continuous variables\n",
    "- we can load our dataset in python from a csv file using pandas and explore it using different plotting types\n",
    "    - normally we would have to preprocess our data\n",
    "- data is split up into training and test sets so that we can evaluate the model's generalization capacity on unseen data\n",
    "- models are trained on the training set and their performance evaluated on the test set\n",
    "- it is generally a good idea to use different metrics when evaluating a model \n",
    "    - this leads to a better understanding of how the model performs\n",
    "\n",
    "\n",
    "## 7. Exercises\n",
    "### Let us now use the different columns to predict if a subject is above the age of 40 or not. \n",
    "Going through these exercises, you will develope a better undertanding of how to train and test models given a dataset. We suggest that you use the above code as a reference but do **NOT** simply copy and paste. You will gain a deeper understanding if your type the code yourself, implement the functions, and use docs to understand how functions work and what parameters to pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add diabetes as another input variable\n",
    "df_clean = df_clean.rename(columns={\"Outcome\":\"Diabetes\"})\n",
    "# Make age the new 'Outcome' variable\n",
    "df_clean[\"Outcome\"] = (df_clean[\"Age\"] >= 40).astype(int)\n",
    "df_clean = df_clean.drop(columns=[\"Age\"])\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Create the new X (features) and y (labels) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "X = df_clean[[\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Diabetes\"]].values\n",
    "y = df_clean[[\"Outcome\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Plot the new label's distribution or counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Play around with the plt methods and parameter to see how it changes the \n",
    "#       display of your plots.\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Split your data into training and test sets\n",
    "\n",
    "You should use the train_test_split function imported from sklearn. We want you to use a test set size of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Once you have finished all the exercises, vary percent of data used for the test set\n",
    "#       and see how it affects the performance of your models.\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Train your Models\n",
    "\n",
    "Implement the non-linear SVC and Logistic Regression models and fit them to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Remember to compute the probabilities for SVC and make sure that the logistic regression \n",
    "#       model converges (by increasing the max_iter argument if necessary).\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Evaluate your Models\n",
    "\n",
    "#### 7.5.1 Predict the labels of the test set using both classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the predict method from the classifier class\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.3 Compute and print the Accuracy both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the score method from each model class.\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.3 Plot the confusion matrices from each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the plot_confusion_matrix function import from sklearn\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.4 Plot the ROC curves and compute the AUC for both classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use the roc_auc_score and roc_curve functions\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "380.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
