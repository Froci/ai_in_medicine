{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Getting started with Deep Learning"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Tutors: Fabian Eitel (Fabian.Eitel@charite.de) and Talia Kimber"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1. Aims of this session\n\nGet a rough idea of how artifical neural networks (ANNs) work, how an implementation in Keras looks like and how suitable they are for tabular data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Learning goals\n\n\n## Theory\n\n* Building blocks of ANNs\n* Model training\n\n## Practical\n\n* Learn to understand the basics using the Tensorflow playground\n* Learn to read a model defintion in Python using Keras\n* Run a pipeline of an ANN on the ADNI tabular data\n* Investigate what filters learn at different layers"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# References\n\n* Stanford Course on Deep Learning http://cs231n.github.io/"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Theory\n\n\n### Building blocks of artificial neural networks\nShowing some of the blocks that can be used when training neural networks and some widely used examples.\n\n__Layer types__\n* Fully connected/linear/dense layers\n* Convolutional layers\n* Pooling layers and other down/upsampling layers\n* Utility layers like input and output layers\n* Batch normalization\n\n__Activation types__\n* Sigmoid\n* Linear\n* Tanh\n* ReLU\n* Leaky ReLU and other variants\n\n__Regularizers__\n* L1 regularization (used in LASSO)\n* L2 regularization / almost the same as weight decay (used in Ridge regression)\n* Dropout\n* Early stopping\n\n__Data functions__\n* Normalization (e.g. using mean and standard deviation)\n* Data augmentation\n* Feature reduction (e.g. Principal Component Analysis [PCA])\n\n__Cost functions__\nCost functions depend on your type of analysis, i.e. regression, binary classification, multi-class classification etc.\n* Softmax\n* Cross-entropy\n* Binary cross-entropy\n* Kullback-Leibler Divergence\n* Smooth losses\n* Mean-squared error\n\nFor more information on each topic view the course link in the references."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 2. Playground excersises"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__Introduction__\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "https://playground.tensorflow.org"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Tensorflow playground is a neural network framework you can use in your browser. Unlike the name says its not based on the popular Tensorflow program. It allows to get some intuition on neural network workings."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note: The links provided here always contain the right settings for the exercise. Always use the corresponding link."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.1 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Use the XOR dataset with 1 hidden layer and try out different activation functions:\n\n\nhttps://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.82689&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.2 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What happens if you add more features one by one?\nStart with X12. Maybe you can add extra layers and neurons too.\n\nhttps://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.82689&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.3 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try a different dataset. Investigate the effects of the learning rate on the training results:\n\nhttps://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=4,2&seed=0.19504&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Real data is never this clean, it is usually were noisy. Now, use the same model from above and add some noise to the data distribution (middle slider on the bottom left). How does it affect the data (you see it on the right) and your model performance?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.4 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After you have added the noise, try out L1 and L2 regularization. What does it do?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.5 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this example the the network fluctuates a lot for some time. What would you change to reduce that effect?\n\nhttps://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.3&regularizationRate=0&noise=25&networkShape=4,2&seed=0.84469&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.6 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Use everything you have learned so far on the more challenging spiral data:\n\nhttps://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=25&networkShape=4,2&seed=0.07992&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n\nHere is an example:\n\nhttps://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.03&noise=25&networkShape=5,4,2&seed=0.16124&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***First break***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3. Practical part"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Preparation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install sklearn --upgrade",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import required packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import balanced_accuracy_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sklearn\nsklearn.__version__",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load data table\ndf = pd.read_csv(\"data/alzheimers_disease_rand.csv\")\n# Print first 5 rows\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Using only year 1 after follow up (month 12) and removing all MCI subjects\ndf = df[df.VISCODE == \"m12\"]\ndf = df[df.DX != \"MCI\"]\n# remove errors from labels\ndf = df[df.DX != \"1600371\"][df.DX != \"1846260\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df[\"DX\"].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Remove rows containing N/A\ndf = df.dropna(subset=[\"Hippocampus\", \"DX\", \"Ventricles\", \"WholeBrain\", \"MMSE\", \"AGE\", \"PTGENDER\"]) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Data splitting"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get an array with the number of samples\nindices = np.arange(len(df))\nprint(\"Order before shuffling: %s\" % indices[:5])\n\n# Shuffle that array\nnp.random.seed(42) # fix a seed so each random event can be repeated\nnp.random.shuffle(indices)\nprint(\"Order after shuffling: %s\"  % indices[:5])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Take first 80% as a training set\nlen_training = int(len(indices) * 0.8) # use int() function to remove decimals\nprint(\"Number of samples for training set: %i\" % len_training)\n\n# Select the first 80% indices\ntrain_idx = indices[0:len_training] # pick 0 to the value of len_training from the indices array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Take the remaining data and split it 50/50\nremaining_samples = len(indices) - len_training\nlen_validation = int(np.ceil(remaining_samples/2)) # round up once\nlen_test = int(np.floor(remaining_samples/2)) # round down once\n\n# Select from the indices array the individual groups\nvalidation_idx = indices[len_training:len_training+len_validation]\ntest_idx = indices[len_training+len_validation:len(indices)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Number of training samples: %i\" % len(train_idx))\nprint(\"Number of validation samples: %i\" % len(validation_idx))\nprint(\"Number of test samples: %i\" % len(test_idx))\nprint(\"Total number of samples: %i\" % (len(train_idx) + len(validation_idx) + len(test_idx)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Feature selection\nWe have to select relevant features for our model to use. Features, which are clinically relevant, should improve the performance of the model. Nevertheless, some features might decrease the performance of the model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our pre-selected list of possible features is: [\"Hippocampus\", \"Ventricles\", \"WholeBrain\", \"MMSE\", \"AGE\", \"PTGENDER\"]\nYou can always come back to this section and try some different ones. Below is some code to use three of them."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = df[[\"Hippocampus\", \"AGE\", \"Ventricles\"]] # You can add or remove features here\n#X.insert(column=\"SEX\", value=(df[\"PTGENDER\"]==\"Male\"), loc=2) # Categorical features like sex need to be translated into numerical values. You can uncomment this line to try it.\n\ny = pd.get_dummies(df[\"DX\"])[\"Dementia\"]\n\nX = X.reset_index(drop=True)\ny = y.reset_index(drop=True)\n\n# Normalize the data such that all features lay on the same range. We substract the mean of each column and divide by the standard deviation.\nscaler = StandardScaler()\nscaler.fit(X.loc[train_idx])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Baseline"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We will start of with a simple Support Vector Machine/Classifier (SVM/SVC)\nclassifier = SVC(C=1, kernel='linear')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "classifier.fit(X=scaler.transform(X.loc[train_idx]), y=y.loc[train_idx])\n#classifier.fit(X=X.loc[train_idx], y=y.loc[train_idx])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Training prediction\ny_pred = classifier.predict(X=scaler.transform(X.loc[train_idx]))\nprint(balanced_accuracy_score(y_true=y.loc[train_idx], y_pred=y_pred))\n\n# Validation prediction\ny_pred = classifier.predict(X=scaler.transform(X.loc[validation_idx]))\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is our baseline performance which we compare to. Normally you would try to optimize this as well first, but that is not the goal of todays lession."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Artifical Neural Network"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing required packages for neural networks\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import load_model\nfrom keras.callbacks import EarlyStopping",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a simple 2 layer neural network\nmodel = Sequential()\nmodel.add(Dense(units=8, activation='relu', input_dim=X.shape[1])) # Hidden layer\nmodel.add(Dense(units=1, activation='sigmoid')) # Output layer\n# Print the model \nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.1 Exercise\nWe want to classify between healthy controls and Alzheimer's patients. This makes two possible output classes i.e. labels 0 (HC) and 1 (AD). What kind of loss function would be suitable for this task?\nIn part 1 you can find different examples and in the Keras documentation you can find how to use them:\nhttps://keras.io/losses/\n\nNote that the program won't fail here yet if you choose the wrong one but only later."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Compile the model with an optimizer, a metric and a loss function\nlearning_rate = 0.03\nopti = Adam(lr=learning_rate)\nmodel.compile(optimizer=opti, loss='@@@ TO DO @@@', metrics=['accuracy']) # replace the TO DO with the correct loss function",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Exercise\nApply the neural network on our Alzheimer's data. Here we will train for only 5 epochs."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.fit(x=scaler.transform(X.loc[train_idx]),\n          y=y.loc[train_idx],\n          epochs=5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The result we obtained above is the training performance, next we will look at the validation performance."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict on the validation set\ny_pred_list = []\nfor idx, row in X.loc[validation_idx].iterrows():\n    x = scaler.transform(row.values.reshape(1, -1)) # scale the data\n    y_pred = model.predict(x)[0][0] # forward pass through the neural network\n    y_pred_list.append(y_pred >= 0.5) # create a list and change the format\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred_list)) # calculate the performance",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***Second break***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As an intermission let's have a look at what the parameters in neural networks actually learn:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "https://www.youtube.com/watch?v=AgkfIQ4IGaM"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "https://scs.ryerson.ca/~aharley/vis/conv/"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 Exercise\n\nNow that we got everything running, let's try to optimize our model!\n\nLet's train the network for more epochs and see how it changes our results."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First create a new instance of the model with un-trained parameters."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a simple 2 layer neural network\nmodel = Sequential()\nmodel.add(Dense(units=8, activation='relu', input_dim=X.shape[1])) # Hidden layer\nmodel.add(Dense(units=1, activation='sigmoid')) # Output layer\n# Print the model \nmodel.summary()\n# Compile the model with an optimizer, a metric and a loss function\nlearning_rate = 0.03\nopti = Adam(lr=learning_rate)\nmodel.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy']) # replace the TO DO with the correct loss function",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.fit(x=scaler.transform(X.loc[train_idx]),\n          y=y.loc[train_idx],\n          epochs=@@@ TODO @@@)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict on the validation set\ny_pred_list = []\nfor idx, row in X.loc[validation_idx].iterrows():\n    x = scaler.transform(row.values.reshape(1, -1)) # scale the data\n    y_pred = model.predict(x)[0][0] # forward pass through the neural network\n    y_pred_list.append(y_pred >= 0.5) # create a list and change the format\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred_list)) # calculate the performance",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can run the above 3 cells again each time you change the number of epochs. This way you can find a good hyperparameter without writing new lines of code.\n\nWhat happens when you run it for a very long time, like 100 epochs? Run the above code again to find out."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.4 Exercise\n\nMake the network wider by increasing the number of units in the hidden layer. You will need to re-compile the network again."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a simple 2 layer neural network\nmodel = Sequential()\nmodel.add(Dense(units=@@@ TO DO @@@, activation='relu', input_dim=X.shape[1])) # Hidden layer\nmodel.add(Dense(units=1, activation='sigmoid')) # Output layer\n# Print the model \nmodel.summary()\n# Compile the model with an optimizer, a metric and a loss function\nlearning_rate = 0.03\nopti = Adam(lr=learning_rate)\nmodel.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model.fit(x=scaler.transform(X.loc[train_idx]),\n          y=y.loc[train_idx],\n          epochs=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict on the validation set\ny_pred_list = []\nfor idx, row in X.loc[validation_idx].iterrows():\n    x = scaler.transform(row.values.reshape(1, -1)) # scale the data\n    y_pred = model.predict(x)[0][0] # forward pass through the neural network\n    y_pred_list.append(y_pred >= 0.5) # create a list and change the format\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred_list)) # calculate the performance",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again you should re-run the 3 cells several times to try out different settings."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.5 Exercise\n\nThis time lets, add more depth to our network by adding more layers."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a simple 2 layer neural network\nmodel = Sequential()\nmodel.add(Dense(units=8, activation='relu', input_dim=X.shape[1])) # Note: input_dim needs to be set on the first layer only\n@@@ TODO @@@ # replace this with another dense layer, which activation do you choose and how many units? You can also add more than one extra layer!\nmodel.add(Dense(units=1, activation='sigmoid')) # Output layer\n# Print the model \nmodel.summary()\n# Compile the model with an optimizer, a metric and a loss function\nlearning_rate = 0.03\nopti = Adam(lr=learning_rate)\nmodel.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model.fit(x=scaler.transform(X.loc[train_idx]),\n          y=y.loc[train_idx],\n          epochs=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict on the validation set\ny_pred_list = []\nfor idx, row in X.loc[validation_idx].iterrows():\n    x = scaler.transform(row.values.reshape(1, -1)) # scale the data\n    y_pred = model.predict(x)[0][0] # forward pass through the neural network\n    y_pred_list.append(y_pred >= 0.5) # create a list and change the format\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred_list)) # calculate the performance",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again you should re-run the 3 cells several times to try out different settings."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.6 Extra exercise\n\nOpen exercise: What else could you change? The learning rate? Can you add some regularization? Different layers with different activations? Try what comes to your mind and get some inspiration from the documentation.\n\nHint: for regularization lookt at https://keras.io/regularizers/"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a simple 2 layer neural network\nmodel = Sequential()\nmodel.add(Dense(units=8, activation='relu', input_dim=X.shape[1])) # Hidden layer, input_dim needs to be set on the first layer only\nmodel.add(Dense(units=1, activation='sigmoid')) # Output layer\n# Print the model \nmodel.summary()\n# Compile the model with an optimizer, a metric and a loss function\nlearning_rate = 0.03\nopti = Adam(lr=learning_rate)\nmodel.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model.fit(x=scaler.transform(X.loc[train_idx]),\n          y=y.loc[train_idx],\n          epochs=20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict on the validation set\ny_pred_list = []\nfor idx, row in X.loc[validation_idx].iterrows():\n    x = scaler.transform(row.values.reshape(1, -1)) # scale the data\n    y_pred = model.predict(x)[0][0] # forward pass through the neural network\n    y_pred_list.append(y_pred >= 0.5) # create a list and change the format\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred_list)) # calculate the performance",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Before we go.."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How does the model perform on the test dataset? In the end you perform this computation only once, with the best model you have found on the training and validation set. Run those cells again which gave you the best model. That way our _model_ variable will be the best one."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Predict on the test set\ny_pred_list = []\nfor idx, row in X.loc[test_idx].iterrows():\n    x = scaler.transform(row.values.reshape(1, -1)) # scale the data\n    y_pred = model.predict(x)[0][0] # forward pass through the neural network\n    y_pred_list.append(y_pred >= 0.5) # create a list and change the format\nprint(balanced_accuracy_score(y_true=y.loc[test_idx], y_pred=y_pred_list)) # calculate the performance",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}