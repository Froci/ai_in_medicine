{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI in Medicine: Data Science - Basics IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Programming: Deep Learning using *Keras*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instructor:* Moritz Seiler, AG Ritter, Charité - Universitätsmedizin Berlin (moritz.seiler@charite.de) <br>\n",
    "*Target audience:* Medical students from Charité<br>\n",
    "*Course date:* July 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, please make sure, that you're using the Python 3.6 kernel!**<br> You can check that in the top right corner of your browser window. If you're using a different Version, go to the Tab *Kernel* --> *Change Kernel* and select *Python 3.6*.\n",
    "\n",
    "**TensorFlow Update:**<br>\n",
    "The pre-installed version of the tensorflow in this environment is 1.1.0, but the following notebook requires version 2.0.0. Please update the tensorflow library by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aims of this session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces **deep learning** in Python with ***Keras***. It is meant to be an **introduction to the *Keras* library** and **<ins>not</ins> a deep dive into the theory** surrounding deep learning. The goal of this session is that you will be able to implement your own neural network in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning\n",
    "- Definition\n",
    "- Fully-connected Neural Networks\n",
    "- Activation functions\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Deep Learning Playground\n",
    "2. Data set\n",
    "3. Keras\n",
    "    - Introduction\n",
    "    - Workflow\n",
    "        - Prepare data\n",
    "        - Build model\n",
    "        - Compile model\n",
    "        - Fit model\n",
    "        - Predict\n",
    "        - Evaluate model\n",
    "        - Save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep Learning<br>\n",
    "https://www.deeplearningbook.org/<br>\n",
    "https://cs231n.github.io/\n",
    "\n",
    "- Keras<br>\n",
    "https://keras.io/guides/<br>\n",
    "https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "- Data<br>\n",
    "https://www.kaggle.com/uciml/pima-indians-diabetes-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See accompanying slide deck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Playground excersises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tensorflow* Playground is an interactive visualisation of neural networks in your browser. In this playground, you can choose different hyper-parameters of your neural network and visually observe the training process. This allows you to get some intuition on neural network workings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Play around and get some intution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Let's try a different dataset. Investigate the effects of the learning rate on the training results. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** We now select the spiral dataset and add noise to the training data (noise=50). What do you observe using the following network architecture?<br>\n",
    "https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=50&networkShape=2&seed=0.00906&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using in this tutorial is the Diabetes data set that you have already prepared in the previous tutorial.\n",
    "\n",
    "<ins>Source:</ins><br>\n",
    "*Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas and NumPy library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the Diabetes data set \n",
    "diabetes = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high-level neural networks API, capable of running on top of Tensorflow, Theano, and CNTK. It enables fast experimentation through a high level, user-friendly, modular and extensible API\n",
    "\n",
    "<img src=\"https://keras.io/img/logo.png\" width=\"40%\">\n",
    "\n",
    "<ins>Source:</ins> https://keras.io/img/logo.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an installation of the *Keras* library, you have to enter the following command in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras comes packaged with *TensorFlow 2.0* as `tensorflow.keras`. To start using *Keras*, simply install *TensorFlow 2.0*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keras* is a model-level library, providing high-level building blocks for developing deep learning models. It doesn't handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. \n",
    "\n",
    "Rather than picking one single tensor library and making the implementation of *Keras* tied to that library, *Keras* handles the problem in a modular way, and several different backend engines can be plugged seamlessly into *Keras*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't going to import the whole *Keras* library but only specific classes that we need for our implementation. This will later be done using the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow\n",
    "from tensorflow.keras.xyz import ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training your first simple neural network with *Keras* doesn't require a lot of code, but we're going to start slow, taking it step-by-step, ensuring you understand the process of how to train a network. The workflow in *Keras* consits of the following steps:\n",
    "1. **Prepare data**\n",
    "2. **Build model**\n",
    "3. **Compile model**\n",
    "4. **Fit model**\n",
    "5. **Predict**\n",
    "6. **Evaluate model**\n",
    "7. **Save model**\n",
    "\n",
    "We're now going to introduce all these steps in more detail in the following part of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our neural network, we have to prepare our data. A first step should always be data inspection, which you have already done in the last tutorial. Here, we will split our data in different subsets to train our model and will standardise the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have learnt in the last tutorial, we have to split our dataset in subsets since we do not know the (hidden) data generating distribution to calculate the true error of our model. For this, we split the diabetes dataset into a training set, validation set and test set using the following split 60-20-20 to approximate the true error. We use the `train_test_split()`function from them `sklearn.model_selection` module to perform the splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split the dataset only in a training and test set. Later in the training step, we use *Keras*' functionality to create a validation set from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes[['Glucose','BMI','Age']], diabetes['Outcome'], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many observations are in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training samples: %i\" % len(X_train))\n",
    "print(\"Number of test samples: %i\" % len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tranformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence of neural networks is ususally faster if the **average of each input variable**  over the training set is **close to zero**. In general, any shift of the average input away from zero will bias the updates in a particular direction and thus slow down learning. Additionally, **scaling the inputs** - to have **about the same covariance** - speeds up learning because it helps to balance out the rate at which the weights connected to the input nodes learn. Therefore, we will *standardize* our data \n",
    "\n",
    "$$z=\\frac{x-\\mu}{\\sigma} $$\n",
    "\n",
    "with mean $\\mu$ and standard deviation $\\sigma$. This transformation will speed up and stabilise the training process. We can use the `StandardScaler()` function from the `sklearn.preprocessing` module to perform the standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instatiate scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train scaler on the data set\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform datasets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at our transformed training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also check the mean and standard deviation of our standardised training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(X_train,0))\n",
    "print(np.std(X_train,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structures of *Keras* are **layers** and **models**. There are three ways to create Keras models:\n",
    "\n",
    "- The Sequential model is simply a linear stack of layers. *(for simple implementations)*\n",
    "- The Functional API fully-featured API that supports arbitrary model architectures. *(for more complex implementations)*\n",
    "- Model subclassing, where you implement everything from scratch on your own. *(for more complex implementations)*\n",
    "\n",
    "We are going to use the **Sequential model** class in this tutorial. You simply need to import the model from the *Keras* library and initialise a model object using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can stack layers and add those to our model object. Before we do so, the implementation of the layer types  \n",
    "- dense layer and \n",
    "- convolutional layer \n",
    "\n",
    "is introduced, which you are going to use later in this notebook. A complete list of supported layer types is given in the *Keras* documentation https://keras.io/api/layers/. First, we have to import these layers from the *Keras* layer module using the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense layer**<br>\n",
    "A dense layer/fully-connected layer is a layer in which each input neuron is connected to each output neuron, the parameters units just tells you the dimensionality of your output. \n",
    "\n",
    "```python\n",
    "Dense(units='dimensionality of the output space',\n",
    "      activation='activation function to use',\n",
    "      ...)\n",
    "```\n",
    "<ins>Example:</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dense(units=6, activation='relu',...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional layer**<br>\n",
    "The convolutional layer’s parameters consist of a set of learnable filters. During the forward pass, we convolve each filter across the width and height of the input and compute dot products between the entries of the filter and the input at any position. By doing so, we produce an activation map that gives the responses of that filter at every spatial position.\n",
    "\n",
    "```python\n",
    "Conv2D(filters='the dimensionality of the output space (i.e. the number of output filters in convolution)', \n",
    "       kernel_size='specifying the height and width of the 2D convolutional kernel', \n",
    "       strides=' specifying the strides of the convolution along the height and width', \n",
    "       activation='activation function to use',\n",
    "       ...)\n",
    "```\n",
    "<ins>Example:</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), activation='relu',...)\n",
    "# if width and height are equal you can write in short\n",
    "Conv2D(filters=16, kernel_size=3, strides=1, activation='relu',...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply add these layers to the Sequential model using the `add()` method.\n",
    "Here, we add  \n",
    "- an dense input layer with 3 units, \n",
    "- a dense sigmoid output layer with 1 unit\n",
    "\n",
    "to create a **fully-connected neural network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Input(shape=(3,)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the content of our neural network by calling the `summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why does our network have 4 parameters?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our neural network, we need to compile our model. This requires us to define a loss function and an optimiser to do so. A list already implemented losses and optimisers can be found in the documentary:\n",
    "\n",
    "- losses: https://keras.io/api/losses/,\n",
    "- optimisers: https://keras.io/api/optimizers/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training. \n",
    "This function will essentially calculate how poorly our model is performing by comparing what the model is predicting with the actual value it is supposed to output. Therefore, the selection of the loss function depends on the the task.\n",
    "\n",
    "<ins>Examples:</ins><br>\n",
    "\n",
    "*Classification:*\n",
    "- `BinaryCrossentropy()`<br> Computes the cross-entropy loss between true labels and predicted labels *(binary)*.\n",
    "- `CategoricalCrossentropy()`<br> Computes the cross-entropy loss between true labels and predicted labels *(multi-class)*.\n",
    "- ...\n",
    "\n",
    "*Regression:*\n",
    "- `MeanSquaredError()`<br> Computes the mean of squares of errors between labels and predictions.\n",
    "- ...\n",
    "\n",
    "You can import the loss functions from the *Keras* losses module by entering the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, MeanSquaredError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimiser**\n",
    "\n",
    "We have already dealt with the loss function, which is a mathematical way of measuring how wrong your predictions are. In other words, the loss function lets us quantify the quality of any particular set of model parameters. The goal of optimisation is to find those parameteres that minimize the loss function. For this, we can compute the best direction along which we should change our parameters that is mathematically guaranteed to be the direction of the steepest descend. This direction will be related to the gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Examples:</ins>\n",
    "- `SGD()`<br> Stochastic gradient descent is an implementation of gradient descent - that you know from the lecture - on batches of observations or single random examples on each pass.\n",
    "\n",
    "- `Adam()`<br> Adam stands for adaptive moment estimation, and is another way of using past gradients to calculate current gradients. Adam also utilizes the concept of momentum by adding fractions of previous gradients to the current one.\n",
    "\n",
    "You can import the optimiser from the `tensorflow.keras.optimizers` module by entering the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instatiate the loss function and the optimiser before we pass them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "loss = BinaryCrossentropy()\n",
    "\n",
    "lr = 4e-2 #lr = 0.04\n",
    "optimiser = SGD(learning_rate=lr)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimiser, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling our model, we're now ready to start the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we can simply use the `fit()` method. Here, we just have to provide the *number of epochs*, the *batch size* and the *training dataset* (features and targerts). Instead of providing a separate validation dataset, we use the `validation_split` functionality, which uses the last x% observations of the training dataset before shuffling to create a validation set.\n",
    "\n",
    "At the end of each epoch, the model will iterate over the validation dataset and compute the validation loss and validation metric.\n",
    "\n",
    "The `fit()` method outputs a `History` object; its `History.history` attribute is a record of loss and metrics values (training + validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we use 50 *epochs* and a *batch size* of 64 to train our model. Additionally, we use a validation split of 0.25 such that our overall data split in training, validation and test set is 60-20-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train,\n",
    "                validation_split=0.25,\n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` methods prints the loss and metric values after every epochs, but we can also use the `History` object to visualise those values to get intuition about the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_epochs = np.arange(0,epochs)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,16))\n",
    "ax1.plot(n_epochs, history.history['loss'], label='training loss')\n",
    "ax1.plot(n_epochs, history.history['val_loss'], label='validation loss')\n",
    "ax1.set_ylim(-0.05,1.05)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(n_epochs, history.history['accuracy'], label='training accuracy')\n",
    "ax2.plot(n_epochs, history.history['val_accuracy'], label='validation accuracy')\n",
    "ax2.set_ylim(-0.05,1.05)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can use our model to predict the target variable using the `predict()` method, which returns the probabilities $P(Outcome=1|X,w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not interested in the probability $P(Outcome=1|X,w)$ but simply want to predict the class label, you can use the `predict_classes()` method. This method uses a threshold of 0.5 to classify examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how good/bad those predictions are, we've to evaluate them using the built-in `evaluate()` method. This returns the loss and metrics values for the model in test mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, y_test, \n",
    "                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to loss and metric values, we're going to use the receiver operating characteristics (ROC) and the area under the curve (AUC) to evaluate our model. We can use the `roc_curve()` and `auc` function from the `sklearn.metrics` module to calculate those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "auc_keras = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating all values, we can use *matplotllib* to plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,8))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_title('ROC curve')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Keras model consists of multiple components:\n",
    "\n",
    "- An architecture, or configuration, which specifyies what layers the model contains, and how they're connected.\n",
    "- A set of weights values (the \"state of the model\").\n",
    "- An optimizer (defined by compiling the model).\n",
    "- A set of losses and metrics (defined by compiling the model).\n",
    "\n",
    "We can save these pieces to disk at once into a single archive in the *Tensorflow* `SavedModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.save('path/to/location')\n",
    "```\n",
    "\n",
    "We can save our neural network to the working directory by entering the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then simply be loaded using the following command:\n",
    "\n",
    "```python\n",
    "model = tensorflow.keras.models.load_model('my_nn')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put everything together**<br>\n",
    "After having itroduced all the steps separately in more detail, everything is put together in one cell, so you don't have to scroll too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "# Data Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes[['Glucose','BMI','Age']], \n",
    "                                                    diabetes['Outcome'], \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Data transformation       \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "## Model building\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(3,)))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "## Model compilation\n",
    "# Loss function\n",
    "loss = BinaryCrossentropy()\n",
    "\n",
    "# Optimiser\n",
    "lr = 4e-2 \n",
    "optimiser = SGD(learning_rate=lr)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimiser, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## Model training\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(x=X_train, y=y_train, \n",
    "                    validation_split=0.25,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "\n",
    "n_epochs = np.arange(0,epochs)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,16))\n",
    "ax1.plot(n_epochs, history.history['loss'], label='training loss')\n",
    "ax1.plot(n_epochs, history.history['val_loss'], label='validation loss')\n",
    "ax1.set_ylim(-0.05,1.05)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(n_epochs, history.history['accuracy'], label='training accuracy')\n",
    "ax2.plot(n_epochs, history.history['val_accuracy'], label='validation accuracy')\n",
    "ax2.set_ylim(-0.05,1.05)\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "## Prediciton\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "## Model evaluation\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, \n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "auc_keras = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,8))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_title('ROC curve')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Save model\n",
    "model.save('my_nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the implementation of a neural network using the *Keras* library was introduced.\n",
    "The workflow to build and train a neural network was presented using a step-by-step introduction.\n",
    "- **Data Preparation:** We splitted the dataset and standardised the input features.\n",
    "- **Model Building:** We created a neural network using the Sequential model class and introduced dense and convolutional layers.\n",
    "- **Model compilaton:** We compiled a neural network and introduced different loss functions and optimisers.\n",
    "- **Model training:** We trained a neural network and visualised the training process.\n",
    "- **Model prediction and evaluation:** We used the trained network to predict the target variable and evaluated the network using different metrics.\n",
    "- **Save model:** We saved all model components in a single archive `SavedModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the basics to implement a neural network in *Keras*, it's time for you to get your hands dirty and get started.\n",
    "\n",
    "*Tip:* The following resources might be helpful:\n",
    "- www.stackoverflow.com\n",
    "- Keras documentation: https://keras.io/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Fully-connected neural network (Basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>**Task:**</ins> Please **implement your own classifier to predict the outcome classes** in the diabetes dataset. <br>\n",
    "<ins>**Model:**</ins> Use a **fully-connected neural network** with at least 1 hidden layer.<br> (*Note:* Since this is still a binary classification task, please use 1 unit and the sigmoid activation in your output layer.)<br>\n",
    "<ins>**Data:**</ins> Use all available data features to predict the outcome class.\n",
    "\n",
    "*Do your results look different from the ones in the example above?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preparation ###\n",
    "# TO DO: Split and transform dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model building ###\n",
    "model = Sequential()\n",
    "# TO DO: Create a neural network with at least 1 hidden layer\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define loss function and optimiser\n",
    "\n",
    "\n",
    "# TO DO: compile model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Choose the training length and batch size\n",
    "\n",
    "history = # TO DO: Train your model (Don't forget validation_split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = np.arange(0,epochs)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,16))\n",
    "ax1.plot(n_epochs, history.history['loss'], label='training loss')\n",
    "ax1.plot(n_epochs, history.history['val_loss'], label='validation loss')\n",
    "ax1.set_ylim(-0.05,1.05)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(n_epochs, history.history['accuracy'], label='training accuracy')\n",
    "ax2.plot(n_epochs, history.history['val_accuracy'], label='validation accuracy')\n",
    "ax2.set_ylim(-0.05,1.05)\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate test loss and accurcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Convolutional neural network (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is to implement a multi-class image classifier for the famous MNIST dataset. This dataset contains 70,000 (28*28 pixel) images of digits 0-9.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"40%\">\n",
    "\n",
    "*Source: https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png*\n",
    "\n",
    "\n",
    "<ins>**Task:**</ins> Please **implement your own multi-class image classifier to predict the outcome classes** in the diabetes dataset. <br>\n",
    "<ins>**Model:**</ins> Use a **convolutional neural network** with at least 1 hidden convolutional layer.<br> (*Note:* Since this is still a multi-class classification task, please use 10 units and the softmax activation in your output layer.)<br>\n",
    "<ins>**Data:**</ins> Use the 28*28 pixel images to predict the digit class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the MNIST data to predict the class labels, we have to **split** the dataset, **normalise** (transform range from 0-255 to 0-1) and reshape the data and **one-hot encode** the target. The data preparation step is already done for you, just execute the following cell and check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST images and labels and normalise (range 0 to 1) image data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # 60,000 training + 10,000 test images/labels\n",
    "X_train = X_train / 255.0\n",
    "X_test  = X_test / 255.0\n",
    "\n",
    "# Reshape dataset to have a single channel \n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)) # The CNN requires this layout (batch_size, height, width, n_channels)\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)) # The CNN requires this layout (batch_size, height, width, n_channels)\n",
    "\n",
    "# One-hot encode target values\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the image data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4, figsize=(12,4))\n",
    "ax1.imshow(X_train[0,:,:,0])\n",
    "ax2.imshow(X_train[1,:,:,0])\n",
    "ax3.imshow(X_train[2,:,:,0])\n",
    "ax4.imshow(X_train[3,:,:,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the one-hot encoded target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the categorical class values, we now have a vector of length equal to the number of categories. The i-th component is equal to 1 if target is assigned to the i-th category and all other components are 0. By using this transformation, we transform the categorical variable into a numerical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to implement your convolutional neural network. The last layers of the network are fully-connected layers with 256 units and ReLu activation function as well as a fully-connected output layer with 10 units and softmax activation function, since we have a classification task with 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(28,28,1)))\n",
    "# TO DO: Add at least 1 convolutional layer\n",
    "\n",
    "model.add(Flatten()) # This flattens your 28*28 image into a vector of length 28*28= 784. \n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(10, activation='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "loss = CategoricalCrossentropy()\n",
    "# TO DO: Define loss function\n",
    "\n",
    "# TO DO: Compile your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Choose the training length and batch size\n",
    "\n",
    "history = # TO DO: Train your model (Don't forget validation_split!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = np.arange(0,20)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,16))\n",
    "ax1.plot(n_epochs, history.history['loss'], label='training loss')\n",
    "ax1.plot(n_epochs, history.history['val_loss'], label='validation loss')\n",
    "ax1.set_ylim(-0.05,1.05)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(n_epochs, history.history['accuracy'], label='training accuracy')\n",
    "ax2.plot(n_epochs, history.history['val_accuracy'], label='validation accuracy')\n",
    "ax2.set_ylim(-0.05,1.05)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate test loss and accurcy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
